{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN AND DEPLOY HOUSE PRICE PREDICTION MODEL, WITH AZURE MACHINE LEARNING SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file includes the following:\n",
    "Create & publish pipeline, using Azure Machine Learning SDK\n",
    "\n",
    "Train & deploy model, using Azure Machine Learning SDK \n",
    "\n",
    "#### REFERENCE DOCS:\n",
    "https://docs.microsoft.com/en-us/learn/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (pywin32 302 (d:\\anaconda\\lib\\site-packages), Requirement.parse('pywin32==227; sys_platform == \"win32\"'), {'docker'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (pywin32 302 (d:\\anaconda\\lib\\site-packages), Requirement.parse('pywin32==227; sys_platform == \"win32\"'), {'docker'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (pywin32 302 (d:\\anaconda\\lib\\site-packages), Requirement.parse('pywin32==227; sys_platform == \"win32\"'), {'docker'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (pywin32 302 (d:\\anaconda\\lib\\site-packages), Requirement.parse('pywin32==227; sys_platform == \"win32\"'), {'docker'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (pywin32 302 (d:\\anaconda\\lib\\site-packages), Requirement.parse('pywin32==227; sys_platform == \"win32\"'), {'docker'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (pywin32 302 (d:\\anaconda\\lib\\site-packages), Requirement.parse('pywin32==227; sys_platform == \"win32\"'), {'docker'}).\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset, Environment, ScriptRunConfig, Experiment\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GET WORKSPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.get(name='aml_workspace',\n",
    "                   subscription_id='0e1c43f5-07fe-4a3e-a2be-743edb639c94',\n",
    "                   resource_group='aml_resource_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UPLOAD DATA TO A DATASTORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"datastore.upload_files\" is deprecated after version 1.0.69. Please use \"FileDatasetFactory.upload_directory\" instead. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspaceblobstore - Default = True\n",
      "workspacefilestore - Default = False\n",
      "workspaceartifactstore - Default = False\n",
      "workspaceworkingdirectory - Default = False\n",
      "Uploading an estimated of 1 files\n",
      "Uploading d:/my_functions/house-prices-advanced-regression-techniques/train.csv\n",
      "Uploaded d:/my_functions/house-prices-advanced-regression-techniques/train.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the default datastore\n",
    "default_ds= ws.get_default_datastore()\n",
    "\n",
    "# Enumerate all datastores, indicating which is the default\n",
    "for ds_name in ws.datastores:\n",
    "    print(ds_name, \"- Default =\", ds_name == default_ds.name)\n",
    "    \n",
    "# Upload data to a datastore\n",
    "default_ds.upload_files(files=['d:/my_functions/house-prices-advanced-regression-techniques/train.csv'], \n",
    "                         overwrite=True, show_progress=True )\n",
    "\n",
    "\n",
    "#Create a tabular dataset from the path on the datastore (this may take a short while)\n",
    "tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'train.csv'))\n",
    "\n",
    "# Register the tabular dataset\n",
    "tab_data_set = tab_data_set.register(workspace=ws, \n",
    "                                        name='housing dataset',\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE COMPUTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Specify a name for the compute (unique within the workspace)\n",
    "compute_name = 'aml-cluster'\n",
    "\n",
    "# Define compute configuration\n",
    "compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2',\n",
    "                                                       min_nodes=0, max_nodes=4,\n",
    "                                                       vm_priority='dedicated')\n",
    "\n",
    "# Create the compute\n",
    "aml_cluster = ComputeTarget.create(ws, compute_name, compute_config)\n",
    "aml_cluster.wait_for_completion(show_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE ENVIRONMENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"databricks\": {\n",
       "        \"eggLibraries\": [],\n",
       "        \"jarLibraries\": [],\n",
       "        \"mavenLibraries\": [],\n",
       "        \"pypiLibraries\": [],\n",
       "        \"rcranLibraries\": []\n",
       "    },\n",
       "    \"docker\": {\n",
       "        \"arguments\": [],\n",
       "        \"baseDockerfile\": null,\n",
       "        \"baseImage\": \"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20211124.v1\",\n",
       "        \"baseImageRegistry\": {\n",
       "            \"address\": null,\n",
       "            \"password\": null,\n",
       "            \"registryIdentity\": null,\n",
       "            \"username\": null\n",
       "        },\n",
       "        \"enabled\": false,\n",
       "        \"platform\": {\n",
       "            \"architecture\": \"amd64\",\n",
       "            \"os\": \"Linux\"\n",
       "        },\n",
       "        \"sharedVolumes\": true,\n",
       "        \"shmSize\": null\n",
       "    },\n",
       "    \"environmentVariables\": {\n",
       "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
       "    },\n",
       "    \"inferencingStackVersion\": null,\n",
       "    \"name\": \"training_environment\",\n",
       "    \"python\": {\n",
       "        \"baseCondaEnvironment\": null,\n",
       "        \"condaDependencies\": {\n",
       "            \"channels\": [\n",
       "                \"anaconda\",\n",
       "                \"conda-forge\"\n",
       "            ],\n",
       "            \"dependencies\": [\n",
       "                \"python=3.6.2\",\n",
       "                {\n",
       "                    \"pip\": [\n",
       "                        \"azureml-defaults\"\n",
       "                    ]\n",
       "                },\n",
       "                \"scikit-learn\",\n",
       "                \"pandas\",\n",
       "                \"numpy\",\n",
       "                \"matplotlib\"\n",
       "            ],\n",
       "            \"name\": \"azureml_24ff5728b2025bb397ffe6f89527df5e\"\n",
       "        },\n",
       "        \"condaDependenciesFile\": null,\n",
       "        \"interpreterPath\": \"python\",\n",
       "        \"userManagedDependencies\": false\n",
       "    },\n",
       "    \"r\": null,\n",
       "    \"spark\": {\n",
       "        \"packages\": [],\n",
       "        \"precachePackages\": true,\n",
       "        \"repositories\": []\n",
       "    },\n",
       "    \"version\": \"6\"\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Creating an environment by specifying packages\n",
    "\n",
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "env = Environment('training_environment')\n",
    "deps = CondaDependencies.create(conda_packages=['scikit-learn','pandas','numpy','matplotlib'],\n",
    "                                pip_packages=['azureml-defaults'])\n",
    "env.python.conda_dependencies = deps\n",
    "\n",
    "#Register environment\n",
    "env.register(workspace=ws)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE SCRIPTS FOR PIPELINE STEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a folder for the script files we'll use in the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create a folder for the pipeline step files\n",
    "\n",
    "os.makedirs('azure_pipeline', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the first script, which will read data from the  dataset and apply some simple pre-processing to remove any rows with missing data and normalize the numeric features so they're on a similar scale.\n",
    "\n",
    "The script includes a argument named --prepped-data, which references the folder where the resulting data should be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting azure_pipeline/perp_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 'azure_pipeline/perp_data.py'\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from azureml.core import Run\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--ds', type=str, dest='dataset_id', help='raw dataset')\n",
    "parser.add_argument('--prepped-data', type=str, dest='prepped_data',  help='Folder for results')\n",
    "args = parser.parse_args()\n",
    "save_folder = args.prepped_data\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the data (passed as an input dataset)\n",
    "print(\"Loading Data...\")\n",
    "dataset = run.input_datasets['my_dataset']\n",
    "train = dataset.to_pandas_dataframe()\n",
    "\n",
    "# Log raw row count\n",
    "row_count = (len(train))\n",
    "run.log('raw_rows', row_count)\n",
    "\n",
    "# Add features\n",
    "train['HouseAge']=train['YrSold']-train['YearBuilt']\n",
    "train['GarageAge']=train['YrSold']-train['GarageYrBlt']\n",
    "train['RemodAge']=train['YearRemodAdd']-train['YearBuilt']\n",
    "\n",
    "#Remove columns\n",
    "dropcols=[ 'YearBuilt', 'YearRemodAdd','GarageYrBlt','YrSold', '3SsnPorch','ScreenPorch', 'MiscVal', 'LowQualFinSF', 'MasVnrArea']\n",
    "train= train.drop(dropcols, axis=1)\n",
    "\n",
    "\n",
    "# Split in categorial and numerical\n",
    "def feature_list(df, dropcols,target, ordlist_cutoff=16):\n",
    "    \"\"\"\n",
    "    Categorising features columns\n",
    "    Arguments: \n",
    "    df - - dataframe\n",
    "    dropcols - - list/single entry of columns that are not considered\n",
    "    ordlist_cutoff - - a number that maximum number of uniques values to be considered for ordinal list\n",
    "    Returns: numcollist and catcollist corresponding to numeric and categorical dtypes\"\"\" \n",
    "\n",
    "    numcollist=[col for col in df.drop(dropcols, axis=1).columns if is_numeric_dtype(df[col]) ]\n",
    "    ordlist=[col for col in numcollist if df[col].nunique() <= ordlist_cutoff ]\n",
    "    for col in ordlist:\n",
    "        numcollist.remove(col)\n",
    "    catcollist=[col for col in df.drop(dropcols, axis=1).columns if not is_numeric_dtype(df[col])]\n",
    "\n",
    "    catcollist=catcollist+ ordlist\n",
    "    numcollist.remove(target)\n",
    "    return numcollist, catcollist\n",
    "\n",
    "df=train\n",
    "target='SalePrice'\n",
    "numcollist, catcollist= feature_list(df, 'Id', target,ordlist_cutoff=16)\n",
    "\n",
    "# DROP OUTLIERS FOR TRAIN SET\n",
    "idx1=train[((train['LotArea']- train['LotArea'].mean())/train['LotArea'].std())>4].index.to_list()\n",
    "idx2=train[((train['LotFrontage']- train['LotFrontage'].mean())/train['LotFrontage'].std())>4].index.to_list()\n",
    "idx3=train[((train['EnclosedPorch']- train['EnclosedPorch'].mean())/train['EnclosedPorch'].std())>8].index.to_list()\n",
    "idx4=train[((train['GarageArea']- train['GarageArea'].mean())/train['GarageArea'].std())>3.5].index.to_list()\n",
    "idx5=train[(((train['GrLivArea']- train['GrLivArea'].mean())/train['GrLivArea'].std())>4.5) &( train['SalePrice']<300000)].index.to_list()\n",
    "idx6=train[((train['1stFlrSF']- train['1stFlrSF'].mean())/train['1stFlrSF'].std())>8].index.to_list()\n",
    "idx7=train[((train['TotalBsmtSF']- train['TotalBsmtSF'].mean())/train['TotalBsmtSF'].std())>10].index.to_list()\n",
    "idx8=train[((train['BsmtFinSF2']- train['BsmtFinSF2'].mean())/train['BsmtFinSF2'].std())>8].index.to_list()\n",
    "idx9=train[((train['BsmtFinSF1']- train['BsmtFinSF1'].mean())/train['BsmtFinSF1'].std())>10].index.to_list()\n",
    "drop_rows_idx = set(idx1+idx2+idx3+idx4+idx5+idx6+idx7+idx8+idx9)\n",
    "\n",
    "train=train.drop(drop_rows_idx, axis=0)\n",
    "\n",
    "# Remove nulls\n",
    "for col in numcollist:\n",
    "    train[col]=train[col].fillna(train[col].mean())\n",
    "\n",
    "# For now we work with the numerical features\n",
    "# Normalize the numeric columns\n",
    "scaler = StandardScaler()\n",
    "train[numcollist]= scaler.fit_transform(train[numcollist])\n",
    "\n",
    "# Log processed rows\n",
    "row_count = (len(train))\n",
    "run.log('processed_rows', row_count)\n",
    "\n",
    "numcollist.append('SalePrice')\n",
    "\n",
    "# Save the prepped data\n",
    "print(\"Saving Data...\")\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "save_path = os.path.join(save_folder,'data.csv')\n",
    "train[numcollist].to_csv(save_path, index=False, header=True)\n",
    "\n",
    "\n",
    "# End the run\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can create the script for the second step, which will train a model. The script includes a argument named --training-data, which references the location where the prepared data was saved by the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting azure_pipeline/training_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 'azure_pipeline/training_data.py'\n",
    "\n",
    "\n",
    "# Import libraries\n",
    "from azureml.core import Run, Model\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--training-data\", type=str, dest='training_data', help='training data')\n",
    "args = parser.parse_args()\n",
    "training_data = args.training_data\n",
    "\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the prepared data file in the training folder\n",
    "print(\"Loading Data...\")\n",
    "file_path = os.path.join(training_data,'data.csv')\n",
    "#shutil.copy('data/diabetes.csv',  file_path)\n",
    "train = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Separate features and labels\n",
    "X=train.drop(['SalePrice'], axis=1)\n",
    "y=train['SalePrice']\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a GB model\n",
    "print('Training a Gradient Boosting Regressor model...')\n",
    "model = GradientBoostingRegressor().fit(X_train, y_train)\n",
    "\n",
    "# evaluate metrics\n",
    "test_pred = model.predict(X_test)\n",
    "rmse=np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "print('Root Mean Squared Error: ', rmse)\n",
    "run.log('Root Mean Squared Error', rmse)\n",
    "\n",
    "r_square= r2_score(y_test, test_pred)\n",
    "print('R square: ', r_square)\n",
    "run.log('R square', r_square)\n",
    "\n",
    "\n",
    "# Plot predicted vs actual\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.scatter(y_test, test_pred)\n",
    "plt.xlabel('Actual Labels')\n",
    "plt.ylabel('Predicted Labels')\n",
    "plt.title('House Price Predictions')\n",
    "plt.plot(y_test,y_test, color='magenta')\n",
    "\n",
    "run.log_image(name = \"Predictions\", plot = fig)\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model in the outputs folder\n",
    "print(\"Saving model...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "model_file = os.path.join('outputs', 'housing_model.pkl')\n",
    "joblib.dump(value=model, filename=model_file)\n",
    "\n",
    "# Register the model\n",
    "print('Registering model...')\n",
    "Model.register(workspace=run.experiment.workspace,\n",
    "               model_path = model_file,\n",
    "               model_name = 'housing_model',\n",
    "              )\n",
    "\n",
    "\n",
    "run.complete()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPARE A COMPUTE ENVIRONMENT FOR THE PIPELINE STEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll use the same compute for both steps, but it's important to realize that each step is run independently; so you could specify different compute contexts for each step if appropriate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "\n",
    "cluster_name = 'aml-cluster'\n",
    "pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "training_env = Environment.get(workspace=ws, name='training_environment')\n",
    "\n",
    "# Create a new runconfig object for the pipeline\n",
    "pipeline_run_config = RunConfiguration()\n",
    "\n",
    "# Use the compute you created above. \n",
    "pipeline_run_config.target = pipeline_cluster\n",
    "\n",
    "# Assign the environment to the run configuration\n",
    "pipeline_run_config.environment = training_env\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE AND RUN A PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to create and run a pipeline.\n",
    "\n",
    "First you need to define the steps for the pipeline, and any data references that need to be passed between them. In this case, the first step must write the prepared data to a folder that can be read from by the second step. Since the steps can be run on remote compute (and in fact, could each be run on different compute), the folder path must be passed as a data reference to a location in a datastore within the workspace. The OutputFileDatasetConfig object is a special kind of data reference that is used for interim storage locations that can be passed between pipeline steps, so you'll create one and use at as the output for the first step and the input for the second step. Note that you need to pass it as a script argument so your code can access the datastore location referenced by the data reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps defined\n"
     ]
    }
   ],
   "source": [
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# Get the training dataset\n",
    "tab_dataset= ws.datasets['housing dataset']\n",
    "\n",
    "# Create an OutputFileDatasetConfig (temporary Data Reference) for data passed from step 1 to step 2\n",
    "prepped_data = OutputFileDatasetConfig(\"prepped_data\")\n",
    "\n",
    "\n",
    "# Step 1, Run the data prep script\n",
    "prep_step = PythonScriptStep(name = \"Prepare Data\",\n",
    "                                source_directory = 'azure_pipeline',\n",
    "                                script_name = 'perp_data.py',\n",
    "                                arguments = ['--ds', tab_dataset.as_named_input('my_dataset'),\n",
    "                                             '--prepped-data', prepped_data],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2, run the training script\n",
    "train_step = PythonScriptStep(name = \"Train and Register Model\",\n",
    "                                source_directory = 'azure_pipeline',\n",
    "                                script_name = 'training_data.py',\n",
    "                                arguments = ['--training-data', prepped_data.as_input()],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "\n",
    "print(\"Pipeline steps defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, you're ready build the pipeline from the steps you've defined and run it as an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built.\n",
      "Created step Prepare Data [8211fdd0][c215ee2b-bd3b-4188-af03-d6e464544fc2], (This step will run and generate new outputs)\n",
      "Created step Train and Register Model [d4b10e4e][c0c9bf49-a412-43f8-b3aa-0d17ed2f84c5], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun ede6e212-cbe1-4694-b743-9ed5040da0f9\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/ede6e212-cbe1-4694-b743-9ed5040da0f9?wsid=/subscriptions/0e1c43f5-07fe-4a3e-a2be-743edb639c94/resourcegroups/aml_resource_group/workspaces/aml_workspace&tid=a49bbff2-fe01-487f-9141-122113ada106\n",
      "Pipeline submitted for execution.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d926533d247945f1a90f11b924b0b88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/ede6e212-cbe1-4694-b743-9ed5040da0f9?wsid=/subscriptions/0e1c43f5-07fe-4a3e-a2be-743edb639c94/resourcegroups/aml_resource_group/workspaces/aml_workspace&tid=a49bbff2-fe01-487f-9141-122113ada106\", \"run_id\": \"ede6e212-cbe1-4694-b743-9ed5040da0f9\", \"run_properties\": {\"run_id\": \"ede6e212-cbe1-4694-b743-9ed5040da0f9\", \"created_utc\": \"2022-01-25T05:19:23.35105Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{}\", \"azureml.continue_on_step_failure\": \"False\", \"azureml.pipelineComponent\": \"pipelinerun\"}, \"tags\": {}, \"end_time_utc\": \"2022-01-25T05:24:03.377728Z\", \"status\": \"Completed\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://amlworkspace5708480350.blob.core.windows.net/azureml/ExperimentRun/dcid.ede6e212-cbe1-4694-b743-9ed5040da0f9/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=6foAKB4GGcRg5TWZVWw3YfOf1TF4vAAWhblIs1Pfe%2FI%3D&skoid=7c70f442-ba12-4608-b995-7cda80df4f94&sktid=a49bbff2-fe01-487f-9141-122113ada106&skt=2022-01-25T03%3A44%3A45Z&ske=2022-01-26T11%3A54%3A45Z&sks=b&skv=2019-07-07&st=2022-01-25T05%3A36%3A36Z&se=2022-01-25T13%3A46%3A36Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://amlworkspace5708480350.blob.core.windows.net/azureml/ExperimentRun/dcid.ede6e212-cbe1-4694-b743-9ed5040da0f9/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=1HG3f8ALhERxDIf1EytO4rukKhIEuLkfPsN8JGGMkt8%3D&skoid=7c70f442-ba12-4608-b995-7cda80df4f94&sktid=a49bbff2-fe01-487f-9141-122113ada106&skt=2022-01-25T03%3A44%3A45Z&ske=2022-01-26T11%3A54%3A45Z&sks=b&skv=2019-07-07&st=2022-01-25T05%3A36%3A36Z&se=2022-01-25T13%3A46%3A36Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://amlworkspace5708480350.blob.core.windows.net/azureml/ExperimentRun/dcid.ede6e212-cbe1-4694-b743-9ed5040da0f9/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=7GextOvG6OVJAwrdkWTmiCX%2BbuzhWso6L18sS4TEv8U%3D&skoid=7c70f442-ba12-4608-b995-7cda80df4f94&sktid=a49bbff2-fe01-487f-9141-122113ada106&skt=2022-01-25T03%3A44%3A45Z&ske=2022-01-26T11%3A54%3A45Z&sks=b&skv=2019-07-07&st=2022-01-25T05%3A36%3A36Z&se=2022-01-25T13%3A46%3A36Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:04:40\", \"run_number\": \"37\", \"run_queued_details\": {\"status\": \"Finished\", \"details\": null}}, \"child_runs\": [{\"run_id\": \"\", \"name\": \"Prepare Data\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}, {\"run_id\": \"\", \"name\": \"Train and Register Model\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2022-01-25 05:19:27Z] Submitting 1 runs, first five are: 8211fdd0:bfe475a1-c725-4356-854b-255c92b9509c\\n[2022-01-25 05:23:25Z] Completing processing run id bfe475a1-c725-4356-854b-255c92b9509c.\\n[2022-01-25 05:23:25Z] Submitting 1 runs, first five are: d4b10e4e:cb991434-9b78-4c5d-8a5a-771d695e0d4c\\n[2022-01-25 05:24:02Z] Completing processing run id cb991434-9b78-4c5d-8a5a-771d695e0d4c.\\n\\nRun is completed.\", \"graph\": {\"datasource_nodes\": {\"8ffe27bb\": {\"node_id\": \"8ffe27bb\", \"name\": \"housing dataset\"}}, \"module_nodes\": {\"8211fdd0\": {\"node_id\": \"8211fdd0\", \"name\": \"Prepare Data\", \"status\": \"NotStarted\"}, \"d4b10e4e\": {\"node_id\": \"d4b10e4e\", \"name\": \"Train and Register Model\", \"status\": \"NotStarted\"}}, \"edges\": [{\"source_node_id\": \"8ffe27bb\", \"source_node_name\": \"housing dataset\", \"source_name\": \"data\", \"target_name\": \"my_dataset\", \"dst_node_id\": \"8211fdd0\", \"dst_node_name\": \"Prepare Data\"}, {\"source_node_id\": \"8211fdd0\", \"source_node_name\": \"Prepare Data\", \"source_name\": \"prepped_data\", \"target_name\": \"input_541f6a5c\", \"dst_node_id\": \"d4b10e4e\", \"dst_node_name\": \"Train and Register Model\"}], \"child_runs\": [{\"run_id\": \"\", \"name\": \"Prepare Data\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}, {\"run_id\": \"\", \"name\": \"Train and Register Model\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.37.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: ede6e212-cbe1-4694-b743-9ed5040da0f9\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/ede6e212-cbe1-4694-b743-9ed5040da0f9?wsid=/subscriptions/0e1c43f5-07fe-4a3e-a2be-743edb639c94/resourcegroups/aml_resource_group/workspaces/aml_workspace&tid=a49bbff2-fe01-487f-9141-122113ada106\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': 'ede6e212-cbe1-4694-b743-9ed5040da0f9', 'status': 'Completed', 'startTimeUtc': '2022-01-25T05:19:26.801672Z', 'endTimeUtc': '2022-01-25T05:24:03.377728Z', 'services': {}, 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'False', 'azureml.pipelineComponent': 'pipelinerun'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://amlworkspace5708480350.blob.core.windows.net/azureml/ExperimentRun/dcid.ede6e212-cbe1-4694-b743-9ed5040da0f9/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=AYHi5xHy2HMbOj8K75iEMJpmd6jz%2BYmg42SPLxZGz%2FU%3D&skoid=7c70f442-ba12-4608-b995-7cda80df4f94&sktid=a49bbff2-fe01-487f-9141-122113ada106&skt=2022-01-25T05%3A09%3A28Z&ske=2022-01-26T13%3A19%3A28Z&sks=b&skv=2019-07-07&st=2022-01-25T05%3A09%3A28Z&se=2022-01-25T13%3A19%3A28Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://amlworkspace5708480350.blob.core.windows.net/azureml/ExperimentRun/dcid.ede6e212-cbe1-4694-b743-9ed5040da0f9/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=Og3JCP0r719SYI5QTmAp63k%2FCBzSZuDW7sNabPSk%2Fl4%3D&skoid=7c70f442-ba12-4608-b995-7cda80df4f94&sktid=a49bbff2-fe01-487f-9141-122113ada106&skt=2022-01-25T05%3A09%3A28Z&ske=2022-01-26T13%3A19%3A28Z&sks=b&skv=2019-07-07&st=2022-01-25T05%3A09%3A28Z&se=2022-01-25T13%3A19%3A28Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://amlworkspace5708480350.blob.core.windows.net/azureml/ExperimentRun/dcid.ede6e212-cbe1-4694-b743-9ed5040da0f9/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=PEd14uV7IsAELmmPiPL6xqSBL2AJxJ87MVLS3SiOG3c%3D&skoid=7c70f442-ba12-4608-b995-7cda80df4f94&sktid=a49bbff2-fe01-487f-9141-122113ada106&skt=2022-01-25T05%3A09%3A28Z&ske=2022-01-26T13%3A19%3A28Z&sks=b&skv=2019-07-07&st=2022-01-25T05%3A09%3A28Z&se=2022-01-25T13%3A19%3A28Z&sp=r'}, 'submittedBy': 'AROONIMA SINHA'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline_steps = [prep_step, train_step]\n",
    "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace=ws, name = 'housing-price-pipeline')\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "RunDetails(pipeline_run).show()\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Register Model :\n",
      "\t Root Mean Squared Error : 28506.082515964164\n",
      "\t R square : 0.8820669915121216\n",
      "\t Predictions : aml://artifactId/ExperimentRun/dcid.cb991434-9b78-4c5d-8a5a-771d695e0d4c/Predictions_1643088229.png\n",
      "Prepare Data :\n",
      "\t raw_rows : 1460\n",
      "\t processed_rows : 1440\n"
     ]
    }
   ],
   "source": [
    "for run in pipeline_run.get_children():\n",
    "    print(run.name, ':')\n",
    "    metrics = run.get_metrics()\n",
    "    for metric_name in metrics:\n",
    "        print('\\t',metric_name, \":\", metrics[metric_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PUBLISH THE PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've created and tested a pipeline, you can publish it as a REST service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Id</th><th>Status</th><th>Endpoint</th></tr><tr><td>diabetes-training-pipeline</td><td><a href=\"https://ml.azure.com/pipelines/78fae10d-e1cf-41e7-8bb0-1db05a2e5393?wsid=/subscriptions/0e1c43f5-07fe-4a3e-a2be-743edb639c94/resourcegroups/aml_resource_group/workspaces/aml_workspace\" target=\"_blank\" rel=\"noopener\">78fae10d-e1cf-41e7-8bb0-1db05a2e5393</a></td><td>Active</td><td><a href=\"https://centralindia.api.azureml.ms/pipelines/v1.0/subscriptions/0e1c43f5-07fe-4a3e-a2be-743edb639c94/resourceGroups/aml_resource_group/providers/Microsoft.MachineLearningServices/workspaces/aml_workspace/PipelineRuns/PipelineSubmit/78fae10d-e1cf-41e7-8bb0-1db05a2e5393\" target=\"_blank\" rel=\"noopener\">REST Endpoint</a></td></tr></table>"
      ],
      "text/plain": [
       "Pipeline(Name: diabetes-training-pipeline,\n",
       "Id: 78fae10d-e1cf-41e7-8bb0-1db05a2e5393,\n",
       "Status: Active,\n",
       "Endpoint: https://centralindia.api.azureml.ms/pipelines/v1.0/subscriptions/0e1c43f5-07fe-4a3e-a2be-743edb639c94/resourceGroups/aml_resource_group/providers/Microsoft.MachineLearningServices/workspaces/aml_workspace/PipelineRuns/PipelineSubmit/78fae10d-e1cf-41e7-8bb0-1db05a2e5393)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Publish the pipeline from the run\n",
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name=\"diabetes-training-pipeline\", description=\"Trains diabetes model\", version=\"1.0\")\n",
    "\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the published pipeline has an endpoint, which you can see in the Endpoints page (on the Pipeline Endpoints tab) in Azure Machine Learning studio. You can also find its URI as a property of the published pipeline object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://centralindia.api.azureml.ms/pipelines/v1.0/subscriptions/0e1c43f5-07fe-4a3e-a2be-743edb639c94/resourceGroups/aml_resource_group/providers/Microsoft.MachineLearningServices/workspaces/aml_workspace/PipelineRuns/PipelineSubmit/78fae10d-e1cf-41e7-8bb0-1db05a2e5393\n"
     ]
    }
   ],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CALL THE PIPELINE ENDPOINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the endpoint, client applications need to make a REST call over HTTP. This request must be authenticated, so an authorization header is required. A real application would require a service principal with which to be authenticated, but to test this out, we'll use the authorization header from current connection to Azure workspace, which you can get using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication header ready.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "auth_header = interactive_auth.get_authentication_header()\n",
    "print(\"Authentication header ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we're ready to call the REST interface. The pipeline runs asynchronously, so we'll get an identifier back, which we can use to track the pipeline experiment as it runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a9b67f00-bb9c-4956-9962-b0aed2cfb976'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "experiment_name = 'housing-price-pipeline'\n",
    "\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=auth_header, \n",
    "                         json={\"ExperimentName\": experiment_name})\n",
    "run_id = response.json()[\"Id\"]\n",
    "run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you have the run ID, you can use it to wait for the run to complete.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: a9b67f00-bb9c-4956-9962-b0aed2cfb976\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/a9b67f00-bb9c-4956-9962-b0aed2cfb976?wsid=/subscriptions/0e1c43f5-07fe-4a3e-a2be-743edb639c94/resourcegroups/aml_resource_group/workspaces/aml_workspace&tid=a49bbff2-fe01-487f-9141-122113ada106\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': 'a9b67f00-bb9c-4956-9962-b0aed2cfb976', 'status': 'Completed', 'startTimeUtc': '2022-01-24T10:35:54.440751Z', 'endTimeUtc': '2022-01-24T10:35:56.540831Z', 'services': {}, 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'Unavailable', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'False', 'azureml.pipelineComponent': 'pipelinerun', 'azureml.pipelineid': '78fae10d-e1cf-41e7-8bb0-1db05a2e5393'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://amlworkspace5708480350.blob.core.windows.net/azureml/ExperimentRun/dcid.a9b67f00-bb9c-4956-9962-b0aed2cfb976/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=Vt4l4OSHusxAiKw1iJ3KSr%2BuEiaMtfObeljCvj0Zudg%3D&skoid=7c70f442-ba12-4608-b995-7cda80df4f94&sktid=a49bbff2-fe01-487f-9141-122113ada106&skt=2022-01-24T03%3A58%3A57Z&ske=2022-01-25T12%3A08%3A57Z&sks=b&skv=2019-07-07&st=2022-01-24T10%3A26%3A10Z&se=2022-01-24T18%3A36%3A10Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://amlworkspace5708480350.blob.core.windows.net/azureml/ExperimentRun/dcid.a9b67f00-bb9c-4956-9962-b0aed2cfb976/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=2Nib9egui3Bwlw7Z1qDSWhbGgcdqUrhdkHoSisnRdx0%3D&skoid=7c70f442-ba12-4608-b995-7cda80df4f94&sktid=a49bbff2-fe01-487f-9141-122113ada106&skt=2022-01-24T03%3A58%3A57Z&ske=2022-01-25T12%3A08%3A57Z&sks=b&skv=2019-07-07&st=2022-01-24T10%3A26%3A10Z&se=2022-01-24T18%3A36%3A10Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://amlworkspace5708480350.blob.core.windows.net/azureml/ExperimentRun/dcid.a9b67f00-bb9c-4956-9962-b0aed2cfb976/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=YtQCBGWiwkjB5CNf%2F%2FP%2B4bIj2cjhNvQkfGEMOBiFow0%3D&skoid=7c70f442-ba12-4608-b995-7cda80df4f94&sktid=a49bbff2-fe01-487f-9141-122113ada106&skt=2022-01-24T03%3A58%3A57Z&ske=2022-01-25T12%3A08%3A57Z&sks=b&skv=2019-07-07&st=2022-01-24T10%3A26%3A10Z&se=2022-01-24T18%3A36%3A10Z&sp=r'}, 'submittedBy': 'AROONIMA SINHA'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.pipeline.core.run import PipelineRun\n",
    "\n",
    "published_pipeline_run = PipelineRun(ws.experiments[experiment_name], run_id)\n",
    "published_pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a schedule for pipeline run with Schedule.create(). Schedule can be created as per fixed time interval(ScheduleRecurrence), or whever there is a change in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEPLOY THE MODEL AS A REAL TIME INFERENCING SERVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to register the model and the same has been done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "model = Model.register(workspace=ws,\n",
    "                       model_name='house_price_prediction_model',\n",
    "                       model_path='housing_model', # local path\n",
    "                       description='A house price pred model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ws.models['housing_model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define the script and environment for the service.\n",
    "A script to load the model and return predictions for submitted data.\n",
    "An environment in which the script will be run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE AN ENTRY SCRIPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the entry script (sometimes referred to as the scoring script) for the service as a Python (.py) file. It must include two functions:\n",
    "\n",
    "init(): Called when the service is initialized.\n",
    "run(raw_data): Called when new data is submitted to the service.\n",
    "Typically, you use the init function to load the model from the model registry, and use the run function to generate predictions from the input data. The following example script shows this pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting azure_pipeline/load_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 'azure_pipeline/load_model.py'\n",
    "\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Called when the service is loaded\n",
    "def init():\n",
    "    global model\n",
    "    # Get the path to the registered model file and load it\n",
    "    #model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.pkl')\n",
    "    #model_file = os.path.join('outputs', 'housing_model.pkl')\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'housing_model.pkl')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "# Called when a request is received\n",
    "def run(raw_data):\n",
    "    # Get the input data as a numpy array\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # Get a prediction from the model\n",
    "    predictions = model.predict(data)\n",
    "    # Return the predictions as any JSON serializable format\n",
    "    return predictions.tolist()\n",
    "    #return predictions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Alternatively\n",
    "def init():\n",
    "    global model\n",
    "    # Get the path to the registered model file and load it\n",
    "    model_path = Model.get_model_path('classification_model')\n",
    "    model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_env = Environment(name='service-env')\n",
    "python_packages = ['scikit-learn', 'numpy', 'pandas','azureml-defaults',  'azure-ml-api-sdk'] # whatever packages your entry script uses\n",
    "for package in python_packages:\n",
    "    service_env.python.conda_dependencies.add_pip_package(package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the script and environment in an InferenceConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "regression_inference_config = InferenceConfig(source_directory = 'azure_pipeline',\n",
    "                                              entry_script='load_model.py',\n",
    "                                                  environment=service_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the deployment configuration and deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model...\n",
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2022-01-24 16:14:53+05:30 Creating Container Registry if not exists.\n",
      "2022-01-24 16:14:54+05:30 Building image..\n",
      "2022-01-24 16:20:53+05:30 Generating deployment configuration.\n",
      "2022-01-24 16:20:54+05:30 Submitting deployment to compute..\n",
      "2022-01-24 16:20:59+05:30 Checking the status of deployment houseprice-service..\n",
      "2022-01-24 16:25:35+05:30 Checking the status of inference endpoint houseprice-service.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Configure the web service container\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
    "\n",
    "# Deploy the model as a service\n",
    "\n",
    "model = ws.models['housing_model']\n",
    "\n",
    "print('Deploying model...')\n",
    "service_name = \"houseprice-service\"\n",
    "service = Model.deploy(ws, service_name, [model], regression_inference_config, deployment_config, overwrite=True)\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "service = Model.deploy(workspace=ws,\n",
    "                       name = 'houseprice-service',\n",
    "                       models = [model],\n",
    "                       inference_config = regression_inference_config,\n",
    "                       deployment_config = deployment_config,\n",
    "                       deployment_target = *production_cluster)\n",
    "service.wait_for_deployment(show_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-24T10:54:44,855216100+00:00 - rsyslog/run \n",
      "2022-01-24T10:54:44,862271100+00:00 - iot-server/run \n",
      "2022-01-24T10:54:44,875589800+00:00 - gunicorn/run \n",
      "Dynamic Python package installation is disabled.\n",
      "Starting HTTP server\n",
      "2022-01-24T10:54:44,918836600+00:00 - nginx/run \n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2022-01-24T10:54:45,283592700+00:00 - iot-server/finish 1 0\n",
      "2022-01-24T10:54:45,285280900+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 20.1.0\n",
      "Listening at: http://127.0.0.1:31311 (72)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 101\n",
      "SPARK_HOME not set. Skipping PySpark Initialization.\n",
      "Initializing logger\n",
      "2022-01-24 10:54:46,393 | root | INFO | Starting up app insights client\n",
      "logging socket was found. logging is available.\n",
      "logging socket was found. logging is available.\n",
      "2022-01-24 10:54:46,401 | root | INFO | Starting up request id generator\n",
      "2022-01-24 10:54:46,401 | root | INFO | Starting up app insight hooks\n",
      "2022-01-24 10:54:46,401 | root | INFO | Invoking user's init function\n",
      "no request id,/azureml-envs/azureml_7a07542250216ae277496d7b33b75e56/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator DummyRegressor from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "\n",
      "no request id,/azureml-envs/azureml_7a07542250216ae277496d7b33b75e56/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "\n",
      "no request id,/azureml-envs/azureml_7a07542250216ae277496d7b33b75e56/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator GradientBoostingRegressor from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "\n",
      "/azureml-envs/azureml_7a07542250216ae277496d7b33b75e56/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator DummyRegressor from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/azureml-envs/azureml_7a07542250216ae277496d7b33b75e56/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/azureml-envs/azureml_7a07542250216ae277496d7b33b75e56/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator GradientBoostingRegressor from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "2022-01-24 10:54:47,258 | root | INFO | Users's init has completed successfully\n",
      "2022-01-24 10:54:47,262 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2022-01-24 10:54:47,265 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2022-01-24 10:54:47,267 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "2022-01-24 10:55:35,713 | root | INFO | Swagger file not present\n",
      "2022-01-24 10:55:35,714 | root | INFO | 404\n",
      "127.0.0.1 - - [24/Jan/2022:10:55:35 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2022-01-24 10:55:43,865 | root | INFO | Swagger file not present\n",
      "2022-01-24 10:55:43,866 | root | INFO | 404\n",
      "127.0.0.1 - - [24/Jan/2022:10:55:43 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for troubleshoot\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at THE workspace in Azure Machine Learning Studio and view the Endpoints page, which shows the deployed services in your workspace.\n",
    "\n",
    "You can also retrieve the names of web services in  workspace by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "houseprice-service\n"
     ]
    }
   ],
   "source": [
    "for webservice_name in ws.webservices:\n",
    "    print(webservice_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE THE WEB SERVICE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the service deployed, now you can consume it from a client application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183302.1255239806]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "x_new = [[-1.71882123,  0.82720887,  0.94761458,  0.69004471, -0.28877977,\n",
    "       -1.14297759, -0.61039742, -0.96816863,  0.51214674, -0.28190747,\n",
    "        0.05274251, -0.42980633, -0.24585151, -0.36365729, -0.68057648,\n",
    "       -0.55502046, -0.47314739]]\n",
    "\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "input_json = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Call the web service, passing the input data (the web service will also accept the data in binary format)\n",
    "predictions = service.run(input_data = input_json)\n",
    "\n",
    "# Get the prediction\n",
    "print(predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above uses the Azure Machine Learning SDK to connect to the containerized web service and use it to generate predictions from your diabetes classification model. In production, a model is likely to be consumed by business applications that do not use the Azure Machine Learning SDK, but simply make HTTP requests to the web service.\n",
    "\n",
    "Let's determine the URL to which these applications must submit their requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://2fd6e687-0935-4f60-bbae-fa78d6c8a179.centralindia.azurecontainer.io/score\n"
     ]
    }
   ],
   "source": [
    "endpoint = service.scoring_uri\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know the endpoint URI, an application can simply make an HTTP request, sending the patient data in JSON format, and receive back the predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[183302.1255239806]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "x_new = [[-1.71882123,  0.82720887,  0.94761458,  0.69004471, -0.28877977,\n",
    "       -1.14297759, -0.61039742, -0.96816863,  0.51214674, -0.28190747,\n",
    "        0.05274251, -0.42980633, -0.24585151, -0.36365729, -0.68057648,\n",
    "       -0.55502046, -0.47314739]]\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "input_json = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Set the content type\n",
    "headers = { 'Content-Type':'application/json' }\n",
    "\n",
    "predictions = requests.post(endpoint, input_json, headers = headers)\n",
    "predictions.json()\n",
    "#predicted = json.loads(predictions.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the service\n",
    "When you no longer need your service, you should delete it to avoid incurring unecessary charges.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service deleted.\n"
     ]
    }
   ],
   "source": [
    "service.delete()\n",
    "print ('Service deleted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
